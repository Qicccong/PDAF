import argparse
import logging
import sys
import time
import math
from bunch import Bunch
import collections

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

import os
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
device = 'cuda' if torch.cuda.is_available() else 'cpu'
from Dataset import Dataset
# from utils import DataLoader

from scipy.sparse import dok_matrix
from evaluate_adv import *
from utils_awp import AdvWeightPerturb1

upper_limit, lower_limit = 1,0

EPS = 1E-20

def cal_lp_norm(tensor,p,dim_count):
    tmp = tensor
    for i in range(1,dim_count):
        tmp = torch.norm(tmp,p=p,dim=i,keepdim=True) #torch.Size([100, 1])
    
    
    return torch.clamp_min(tmp, 1e-8)

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', default='NeuMF') # NeuMF ConvNCF
    parser.add_argument('--l2', default=0, type=float)
    parser.add_argument('--l1', default=0, type=float)
    parser.add_argument('--batch-size', default=100, type=int)
    parser.add_argument("--data_path", nargs="?", default="Data/",
                        help="Input data path.")
    parser.add_argument("--dataset", nargs="?", default="ml-1m",   # ml-1m yelp  AToy lastfm
                        help="Choose a dataset.")
    parser.add_argument('--epochs', default=100, type=int)
    parser.add_argument('--lr-max', default=0.0005, type=float)        # 0.1 è¿™å—åº”è¯¥æ˜¯æ¨èæ¨¡å‹è®­ç»ƒå­¦ä¹ ç‡ RAT ml1m 0.0005
    parser.add_argument('--lr-prox', default=0.05, type=float)
    parser.add_argument('--attack', default='fgsm', type=str, choices=['fgsm', 'fgsm', 'bim', 'none' ,'Gaussian ','Uniform ', 'pgd', 'mim'])
    parser.add_argument('--epsilon', default=0.008 ,type=float) 
    parser.add_argument("--alpha", type=float ,default=0.008/10)  # 0.5/25  nargs="?",
    parser.add_argument("--num_steps", type=int, default=10)  #25
    # parser.add_argument('--norm', default='l_inf', type=str, choices=['l_inf', 'l_2'])
    parser.add_argument('--fname', default='mlp_model', type=str)
    # parser.add_argument('--seed', default=123, type=int)
    # parser.add_argument('--half', action='store_true')
    # parser.add_argument('--width-factor', default=10, type=int)
    parser.add_argument('--eval', action='store_true')
    parser.add_argument('--val', action='store_true')
    parser.add_argument("--decay_factor", nargs="?", default=1.0)
    # parser.add_argument('--chkpt-iters', default=10, type=int)
    # parser.add_argument('--awp-gamma', default=0.00001, type=float)
    # parser.add_argument('--awp-warmup', default=0, type=int)
    parser.add_argument("--fcLayers", nargs="?", default="[1024, 512, 256, 128, 64, 32, 16]", #  [512, 256, 128, 64, 32, 16]  [512,  128, 32]
                        help="Size of each layer. Note that the first layer is the "
                             "concatenation of user and item embeddings. So fcLayers[0]/2 is the embedding size.")
    parser.add_argument("--nNeg", type=int, default=4,help="Number of negative instances to pair with a positive instance.")
    parser.add_argument("--model_path", type=str, default="pretrained/ml-1m_MLP_TWP_23_1e-06_0.001_BEST.pth",help="pretrained/ml-1m_MLP_TWP_23_1e-06_0.001_BEST.pth")
 
    return parser.parse_args()

args = get_args()

class NeuMF(nn.Module):
    def __init__(self, user_num, item_num, embedding_dim_GMF,embedding_dim_MLP, hidden_layer_MLP,
                 dropout,device, GMF_model=None,MLP_model=None):
        super(NeuMF, self).__init__()
        """
        user_num: number of users;
        item_num: number of items;
        embedding_dim_GMF: number of embedding dimensions in GMF submodel;
        embedding_dim_MLP: number of embedding dimensions in MLP submodel;
        hidden_layer_MLP: dimension of each hidden layer (list type) in MLP submodel;
        dropout: dropout rate between fully connected layers;
        GMF_model: pre-trained GMF weights;
		MLP_model: pre-trained MLP weights.
        """
        self.device = device
        self.dropout = dropout
        self.GMF_model = GMF_model
        self.MLP_model = MLP_model
        self.criterion = torch.nn.BCELoss()  # æ·»åŠ æŸå¤±å‡½æ•°
        
        self.embed_user_GMF = nn.Embedding(user_num, embedding_dim_GMF).to(self.device)
        self.embed_item_GMF = nn.Embedding(item_num, embedding_dim_GMF).to(self.device)
        self.embed_user_MLP = nn.Embedding(user_num, embedding_dim_MLP).to(self.device)
        self.embed_item_MLP = nn.Embedding(item_num, embedding_dim_MLP).to(self.device)

        self.MLP_layers = nn.ModuleList()
        self.num_layers = len(hidden_layer_MLP)
        self.relu_outputs = []  # å­˜å‚¨ReLUå±‚çš„è¾“å‡º

        for i in range(self.num_layers):
            # Dropout å±‚
            dropout_layer = nn.Dropout(p=self.dropout)
            self.MLP_layers.append(dropout_layer)
            # Linear å±‚
            if i == 0:
                linear_layer = nn.Linear(embedding_dim_MLP*2, hidden_layer_MLP[0])
            else:
                linear_layer = nn.Linear(hidden_layer_MLP[i-1], hidden_layer_MLP[i])
            self.MLP_layers.append(linear_layer)
            # ReLU å±‚
            relu_layer = nn.ReLU()
            self.MLP_layers.append(relu_layer)

        # é¢„æµ‹å±‚
        self.predict_layer = nn.Linear(hidden_layer_MLP[-1] + embedding_dim_GMF, 1)

        # MLP_modules = []
        # self.num_layers = len(hidden_layer_MLP)
        # for i in range(self.num_layers):
        #     MLP_modules.append(nn.Dropout(p=self.dropout))
        #     if i == 0:
        #         MLP_modules.append(nn.Linear(embedding_dim_MLP*2, hidden_layer_MLP[0]))
        #     else:
        #         MLP_modules.append(nn.Linear(hidden_layer_MLP[i-1], hidden_layer_MLP[i]))
        #     MLP_modules.append(nn.ReLU())
        # self.MLP_layers = nn.Sequential(*MLP_modules)
        # self.predict_layer = nn.Linear(hidden_layer_MLP[-1]+embedding_dim_GMF, 1)

        if True:
            nn.init.normal_(self.embed_user_GMF.weight, std=0.01)
            nn.init.normal_(self.embed_item_GMF.weight, std=0.01)
            nn.init.normal_(self.embed_user_MLP.weight, std=0.01)
            nn.init.normal_(self.embed_item_MLP.weight, std=0.01)

            for m in self.MLP_layers:
                if isinstance(m, nn.Linear):
                    nn.init.xavier_uniform_(m.weight)
            nn.init.kaiming_uniform_(self.predict_layer.weight, a=1, nonlinearity='sigmoid')

            # Kaiming/Xavier initialization can not deal with non-zero bias terms
            for m in self.modules():
                if isinstance(m, nn.Linear) and m.bias is not None:
                    m.bias.data.zero_()
        else:
            # embedding layers
            self.embed_user_GMF.weight.data.copy_(self.GMF_model.embed_user.weight)
            self.embed_item_GMF.weight.data.copy_(self.GMF_model.embed_item.weight)
            self.embed_user_MLP.weight.data.copy_(self.MLP_model.embed_user.weight)
            self.embed_item_MLP.weight.data.copy_(self.MLP_model.embed_item.weight)

            # mlp layers
            for (m1, m2) in zip(self.MLP_layers, self.MLP_model.MLP_layers):
                if isinstance(m1, nn.Linear) and isinstance(m2, nn.Linear):
                    m1.weight.data.copy_(m2.weight)
                    m1.bias.data.copy_(m2.bias)

            # predict layers
            predict_weight =  self.MLP_model.predict_layer.weight
            precit_bias = self.MLP_model.predict_layer.bias

            self.predict_layer.weight.data.copy_(0.5 * predict_weight)
            self.predict_layer.bias.data.copy_(0.5 * precit_bias)

    def forward(self, user, item, label):
        # GMFè®¡ç®—
        embed_user_GMF = self.embed_user_GMF(user).to(self.device)
        embed_item_GMF = self.embed_item_GMF(item).to(self.device)
        output_GMF = embed_user_GMF * embed_item_GMF

        embed_user_MLP = self.embed_user_MLP(user).to(self.device)
        embed_item_MLP = self.embed_item_MLP(item).to(self.device)
        interaction = torch.cat((embed_user_MLP, embed_item_MLP), -1).to(self.device)
        # output_MLP = self.MLP_layers(interaction).to(self.device)
        # concat = torch.cat((output_GMF, output_MLP), -1).to(self.device)

        # prediction = self.predict_layer(concat)
        # return prediction.view(-1)
        
        # é€å±‚è®¡ç®—å¹¶ä¿å­˜ä¸­é—´è¾“å‡º
        x = interaction
        mlp_outputs = []  # å­˜å‚¨MLPä¸­é—´å±‚è¾“å‡º
        # éå†æ‰€æœ‰å±‚ï¼ˆDropout->Linear->ReLU ä¸ºä¸€ç»„ï¼‰
        for i in range(0, len(self.MLP_layers), 3):
            # Dropout
            x = self.MLP_layers[i](x)
            # Linear
            x = self.MLP_layers[i+1](x)
            # ReLU (ä¿å­˜æ­¤å±‚è¾“å‡º)
            x = self.MLP_layers[i+2](x)
            x.requires_grad_(True)
            x.retain_grad()
            mlp_outputs.append(x)
        
        output_MLP = x

        # åˆå¹¶è·¯å¾„
        concat = torch.cat((output_GMF, output_MLP), -1).to(self.device)
        # é¢„æµ‹
        prediction = torch.sigmoid(self.predict_layer(concat)).view(-1)
        # è®¡ç®—æŸå¤±
        loss = self.criterion(prediction, label.float())
        
        # è¿”å›æŸå¤±å’Œä¸­é—´å±‚è¾“å‡º
        return loss, *mlp_outputs, prediction


class ConvNCF(nn.Module):

    def __init__(self, user_count, item_count,device):
        super(ConvNCF, self).__init__()

        # some variables
        self.device = device
        self.user_count = user_count
        self.item_count = item_count
        # self.item_count = 12929 # AMusic
        # æ·»åŠ æŸå¤±å‡½æ•°
        self.criterion = torch.nn.BCEWithLogitsLoss()

        # embedding setting
        self.embedding_size = 64

        self.P = nn.Embedding(self.user_count, self.embedding_size).to(self.device)
        self.Q = nn.Embedding(self.item_count, self.embedding_size).to(self.device)

        # cnn setting
        self.channel_size = 32
        self.kernel_size = 2
        self.strides = 2
        self.cnn = nn.Sequential(
            # batch_size * 1 * 64 * 64
            nn.Conv2d(1, self.channel_size, self.kernel_size, stride=self.strides),
            nn.ReLU(),
            # batch_size * 32 * 32 * 32
            nn.Conv2d(self.channel_size, self.channel_size, self.kernel_size, stride=self.strides),
            nn.ReLU(),
            # batch_size * 32 * 16 * 16
            nn.Conv2d(self.channel_size, self.channel_size, self.kernel_size, stride=self.strides),
            nn.ReLU(),
            # batch_size * 32 * 8 * 8
            nn.Conv2d(self.channel_size, self.channel_size, self.kernel_size, stride=self.strides),
            nn.ReLU(),
            # batch_size * 32 * 4 * 4
            nn.Conv2d(self.channel_size, self.channel_size, self.kernel_size, stride=self.strides),
            nn.ReLU(),
            # batch_size * 32 * 2 * 2
            nn.Conv2d(self.channel_size, self.channel_size, self.kernel_size, stride=self.strides),
            nn.ReLU(),
            # batch_size * 32 * 1 * 1
        )

        self.fc = nn.Linear(32, 1)
        # self.fc = nn.Sequential(
        #     nn.Linear(32, 16),  # è¾“å…¥ç»´åº¦æ”¹ä¸º64
        #     nn.ReLU(),
        #     nn.Dropout(0.5),
        #     nn.Linear(16, 1)
        # )
                

    def forward(self, user_ids, item_ids, label):  # æ·»åŠ labelå‚æ•°
        user_ids = list(map(int, user_ids))
        item_ids = list(map(int, item_ids))

        user_embeddings = self.P(torch.tensor(user_ids).to(self.device))
        item_embeddings = self.Q(torch.tensor(item_ids).to(self.device))

        interaction_map = torch.bmm(user_embeddings.unsqueeze(2), 
                                  item_embeddings.unsqueeze(1)).to(self.device)
        interaction_map = interaction_map.view((-1, 1, self.embedding_size, self.embedding_size))

        # å­˜å‚¨ä¸­é—´å±‚è¾“å‡º
        intermediate_outputs = []
        x = interaction_map
        
        # æ‰‹åŠ¨è®¡ç®—æ¯ä¸ªæ¨¡å—çš„è¾“å‡º
        for module in self.cnn:
            x = module(x)
            if isinstance(module, nn.ReLU):
                # ä¿å­˜ReLUè¾“å‡º
                x.requires_grad_(True)
                x.retain_grad()
                intermediate_outputs.append(x)
        
        # ç‰¹å¾å±•å¹³
        feature_vec = x.view((-1, 32))
        
        # å…¨è¿æ¥å±‚é¢„æµ‹
        prediction = torch.sigmoid(self.fc(feature_vec).view(-1))
        prediction = torch.clamp(prediction, 1e-8, 1 - 1e-8)

        logits = self.fc(feature_vec).view(-1)

        if isinstance(label, bool):
            # åœ¨è¯„ä¼°æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬ä¸éœ€è¦è®¡ç®—æŸå¤±
            # æ‰€ä»¥åªéœ€è¿”å›é¢„æµ‹å€¼
            return torch.sigmoid(logits)
        # è®¡ç®—æŸå¤±
        # label = label.to(prediction.device)
        # loss = self.criterion(prediction, label.float())

        label = label.to(logits.device)
        loss = self.criterion(logits, label.float())
        prediction = torch.sigmoid(logits)
        
        return loss, *intermediate_outputs, prediction
    

import torch
import zipfile
import io
import pickle
import os
def robust_load(path, device='cpu'):
    """
    å¥å£®çš„æ¨¡å‹åŠ è½½å‡½æ•°ï¼Œæ”¯æŒå¤šç§åŠ è½½æ–¹å¼å’Œæ¢å¤æœºåˆ¶
    è‡ªåŠ¨å¤„ç†å¸¸è§çš„æ¨¡å‹ä¿å­˜æ ¼å¼
    """
    # å°è¯•åŠ è½½æ•´ä¸ªæ–‡ä»¶
    loaded_obj = None
    error = None
    
    # æ–¹æ³•1: å°è¯•æ ‡å‡†åŠ è½½
    try:
        loaded_obj = torch.load(path, map_location=device, weights_only=False)
        print("âœ… æ ‡å‡†åŠ è½½æˆåŠŸ")
    except Exception as e:
        error = f"æ ‡å‡†åŠ è½½å¤±è´¥: {str(e)}"
    
    # æ–¹æ³•2: å¦‚æœæ ‡å‡†åŠ è½½å¤±è´¥ï¼Œå°è¯•å†…éƒ¨åŠ è½½å™¨
    if loaded_obj is None:
        try:
            with open(path, 'rb') as f:
                # æä¾›å®Œæ•´çš„å‚æ•°
                loaded_obj = torch.serialization._load(
                    f, 
                    map_location=device,
                    pickle_module=pickle,
                    weights_only=False
                )
            print("âœ… å†…éƒ¨åŠ è½½å™¨æˆåŠŸ")
        except Exception as e:
            error = error or f"å†…éƒ¨åŠ è½½å¤±è´¥: {str(e)}"
    
    # æ–¹æ³•3: æ‰‹åŠ¨å¤„ç†ZIPå†…å®¹ï¼ˆå¦‚æœå‰ä¸¤ç§å¤±è´¥ï¼‰
    if loaded_obj is None:
        try:
            with zipfile.ZipFile(path, 'r') as zf:
                for name in zf.namelist():
                    if 'data.pkl' in name or name.endswith('.pkl'):
                        with zf.open(name) as f:
                            data = f.read()
                        loaded_obj = torch.load(io.BytesIO(data), map_location=device, weights_only=False)
                        break
            if loaded_obj is not None:
                print("âœ… ZIPå¤„ç†æˆåŠŸ")
        except Exception as e:
            error = error or f"ZIPå¤„ç†å¤±è´¥: {str(e)}"
    
    # æ–¹æ³•4: æš´åŠ›è§£ææ•´ä¸ªæ–‡ä»¶ï¼ˆå¦‚æœå‰ä¸‰ç§å¤±è´¥ï¼‰
    if loaded_obj is None:
        try:
            with open(path, 'rb') as f:
                data = f.read()
                # å°è¯•ç›´æ¥ååºåˆ—åŒ–
                try:
                    loaded_obj = torch.load(io.BytesIO(data), map_location=device, weights_only=False)
                except Exception as e:
                    pass
                
                # å°è¯•pickleåŠ è½½
                if loaded_obj is None:
                    try:
                        loaded_obj = pickle.loads(data)
                    except Exception as e:
                        pass
                    
            if loaded_obj is not None:
                print("âœ… æš´åŠ›è§£ææˆåŠŸ")
        except Exception as e:
            error = error or f"æš´åŠ›è§£æå¤±è´¥: {str(e)}"
    
    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
    if loaded_obj is None:
        raise RuntimeError(f"æ— æ³•åŠ è½½æ¨¡å‹æ–‡ä»¶: {path}\n{error}")
    
    # ================= å…³é”®ä¿®æ”¹ï¼šæ™ºèƒ½æå–çŠ¶æ€å­—å…¸ =================
    # 1. ç›´æ¥å¤„ç†çŠ¶æ€å­—å…¸
    if isinstance(loaded_obj, (dict, collections.OrderedDict)):
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¸¸è§çš„å°è£…æ ¼å¼
        if 'state_dict' in loaded_obj:
            print("ğŸ” æ£€æµ‹åˆ°'state_dict'é”®ï¼Œæå–æ¨¡å‹å‚æ•°")
            return loaded_obj['state_dict']
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ‚¨çš„è‡ªå®šä¹‰æ ¼å¼
        if 'model' in loaded_obj:
            print("ğŸ” æ£€æµ‹åˆ°'model'é”®ï¼Œæå–æ¨¡å‹å‚æ•°")
            return loaded_obj['model']
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å—å‰ç¼€çš„é”®
        has_module_prefix = any('module.' in key for key in loaded_obj.keys())
        if has_module_prefix:
            print("ğŸ” æ£€æµ‹åˆ°åŒ…å«'module.'å‰ç¼€çš„é”®ï¼Œä½œä¸ºçŠ¶æ€å­—å…¸å¤„ç†")
            return loaded_obj
    
    # 2. å¤„ç†DataParallelå°è£…çš„æ¨¡å‹
    elif hasattr(loaded_obj, 'state_dict'):
        print("ğŸ” æ£€æµ‹åˆ°åŒ…å«state_dictæ–¹æ³•çš„å¯¹è±¡ï¼Œä½¿ç”¨è¯¥æ–¹æ³•")
        return loaded_obj.state_dict()
    
    # 3. æ£€æŸ¥ç›´æ¥çŠ¶æ€å­—å…¸ç‰¹å¾
    elif isinstance(loaded_obj, torch.nn.Module):
        print("ğŸ” æ£€æµ‹åˆ°nn.Moduleå¯¹è±¡ï¼Œç›´æ¥è¿”å›å…¶çŠ¶æ€å­—å…¸")
        return loaded_obj.state_dict()
    
    # 4. éæ ‡å‡†æ ¼å¼ä½†å¯èƒ½æ˜¯çŠ¶æ€å­—å…¸
    print("âš ï¸ æ— æ³•è¯†åˆ«æ¨¡å‹ç»“æ„ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨åŠ è½½ç»“æœ")
    return loaded_obj

def main():
    All_start_time = time.time()
    args = get_args()
    fcLayers = eval(args.fcLayers)
    topK = 10
    topK1 = 20
    topK2 = 50
    topK3 = 100
    print("æ¯”ä¾‹l2,æ¯ä¸€è½®æ”¹å˜æ¨¡å‹çš„å‚æ•°éƒ½ä¸å˜,ä¸ç´¯ç§¯å½±å“ä¸‹å»")
    print('--'*50)


    logger = logging.getLogger(__name__)    # è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯é…ç½®ä¸€ä¸ªå…·æœ‰æŒ‡å®šæ ¼å¼å’Œçº§åˆ«çš„æ—¥å¿—è®°å½•å™¨ï¼Œä»¥ä¾¿è®°å½•ç¨‹åºæ‰§è¡Œæ—¶çš„ç›¸å…³ä¿¡æ¯ã€‚æ–‡ä»¶å’Œæ§åˆ¶å°éƒ½è¢«ç”¨ä½œè¾“å‡ºç›®æ ‡ã€‚
    logging.basicConfig(
        format='[%(asctime)s] - %(message)s',
        datefmt='%Y/%m/%d %H:%M:%S',
        level=logging.DEBUG,
        handlers=[
            logging.FileHandler(os.path.join(args.fname, 'eval.log' if args.eval else 'output.log')),
            logging.StreamHandler()
        ])

    logger.info(args)
    
    # Loading data
    t1 = time.time()
    dataset = Dataset(args.data_path + args.dataset)
    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives
    nUsers, nItems = train.shape
    
    arr = np.array(testRatings)
    max_value_second_column1 = np.max(arr[:, 1])
    arr = np.array(testNegatives)
    max_value_second_column2 = np.max(arr)
    n_value = max(max_value_second_column1, max_value_second_column2)
    if nItems <= n_value:
        nItems = n_value+1
        extended_sparse_matrix = dok_matrix((nUsers, nItems), dtype=np.float32)

        for (row, col), value in train.items():
            extended_sparse_matrix[row, col] = value
            train = extended_sparse_matrix
    
    userMatrix = torch.Tensor(get_train_matrix(train))

    itemMatrix = torch.transpose(torch.Tensor(get_train_matrix(train)), 0, 1)
    userMatrix, itemMatrix = userMatrix.to(device), itemMatrix.to(device)
    print(f"Load data: #user={nUsers}, #item={nItems}, #train={train.nnz}, #test={len(testRatings)} [{time.time()-t1:.1f}s]")
    print("MLP_Attack_test:*************************************")

    print(args.model)
    if args.model == "NeuMF":
        GMF_model = None
        MLP_model = None
        model_adv = NeuMF(nUsers, nItems, 32, 32, [64,32,16,8], 0.0, device, GMF_model,MLP_model)
    elif args.model == "ConvNCF":
        model_adv = ConvNCF(nUsers, nItems, device=device)
    else:
        raise ValueError("Unknown model")

    model_adv = nn.DataParallel(model_adv).to(device)

    if args.l2:
        decay, no_decay = [], []
        for name,param in model_adv.named_parameters():
            if 'bn' not in name and 'bias' not in name:
                decay.append(param)
            else:
                no_decay.append(param)
        params = [{'params':decay, 'weight_decay':args.l2},
                  {'params':no_decay, 'weight_decay': 0 }]
    else:
        params = model_adv.parameters()

    model_path = args.model_path

    print(model_path)
    print("attack_weight")
    # if 'robust'or 'MLP' in model_path: 
    if 'robust' in model_path:  
        # å‡è®¾ original_state_dict æ˜¯ä½ åŠ è½½çš„åŸå§‹çŠ¶æ€å­—å…¸ 
        original_state_dict = robust_load(model_path, device)
        # original_state_dict = torch.load(model_path, map_location=device, weights_only=False)
        # # å¦‚æœæ˜¯mlpç”Ÿæˆçš„æ¨¡å‹ï¼Œæœ‰netéœ€è¦æå–å‡ºæ¥
        # original_state_dict = original_state_dict['net']
        
        # ä¿®æ”¹é”®ä»¥åŒ¹é… DataParallel çš„æœŸæœ›æ ¼å¼
        modified_state_dict = {'module.' + key: value for key, value in original_state_dict.items()}

        # åˆ›å»ºä¸€ä¸ªæ–°çš„state_dictï¼ŒåªåŒ…å«ä¸å¸¦æœ‰'reg'å­—æ®µçš„å‚æ•°
        new_state_dict111 = {}
        for key, value in modified_state_dict.items():
            if 'reg' not in key:
                new_state_dict111[key] = value
        model_adv.load_state_dict(new_state_dict111)
        
        # # ç°åœ¨ä½¿ç”¨ä¿®æ”¹åçš„çŠ¶æ€å­—å…¸åŠ è½½æ¨¡å‹
        # model_adv.load_state_dict(modified_state_dict) 
    else:  # RAWP
        original_state_dict = robust_load(model_path, device)
        # original_state_dict = torch.load(model_path, map_location=device, weights_only=False)
        # åˆ›å»ºä¸€ä¸ªæ–°çš„state_dictï¼ŒåªåŒ…å«ä¸å¸¦æœ‰'reg'å­—æ®µçš„å‚æ•°
        modified_state_dict = {'module.' + key: value for key, value in original_state_dict.items()}
        original_state_dict = modified_state_dict

        new_state_dict111 = {}
        for key, value in original_state_dict.items():
            if 'reg' not in key:
                new_state_dict111[key] = value
        model_adv.load_state_dict(new_state_dict111)
        
    del new_state_dict111
    torch.cuda.empty_cache()

    # Check  performance
    t1 = time.time()
    hits10, ndcgs10, maps10, mrrs10 = evaluate_model(model_adv, testRatings, testNegatives, topK,topK1,topK2,topK3,args.attack,args.epsilon,args.alpha,args.num_steps, args.decay_factor, num_thread=1, device=device)
    hr10, ndcg10,map10,mrr10 = np.array(hits10).mean(), np.array(ndcgs10).mean(),np.array(maps10).mean(), np.array(mrrs10).mean()
    print(f"Init: HR10={hr10:.4f}, NDCG10={ndcg10:.4f}, mrrs10={mrr10:.4f}")
    

if __name__ == "__main__":
    main()
